{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Train a PC\n\nThis tutorial demonstrates how to create a Hidden Chow-Liu Tree (https://arxiv.org/pdf/2106.02264.pdf) using `pyjuice.structures` and train the model with mini-batch EM and full-batch EM.\n\nFor simplicity, we use the MNIST dataset as an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = 'imgs/juice.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the MNIST Dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pyjuice as juice\nimport torch\nimport torchvision\nimport time\nfrom torch.utils.data import TensorDataset, DataLoader\nimport pyjuice.nodes.distributions as dists\n\ntrain_dataset = torchvision.datasets.MNIST(root = \"../data\", train = True, download = True)\nvalid_dataset = torchvision.datasets.MNIST(root = \"../data\", train = False, download = True)\n\ntrain_data = train_dataset.data.reshape(60000, 28*28)\nvalid_data = valid_dataset.data.reshape(10000, 28*28)\n\ntrain_loader = DataLoader(\n    dataset = TensorDataset(train_data),\n    batch_size = 512,\n    shuffle = True,\n    drop_last = True\n)\nvalid_loader = DataLoader(\n    dataset = TensorDataset(valid_data),\n    batch_size = 512,\n    shuffle = False,\n    drop_last = True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the PC\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a HCLT PC with latent size 128.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n\n# The data is required to construct the backbone Chow-Liu Tree structure for the HCLT\nns = juice.structures.HCLT(\n    train_data.float().to(device), \n    num_latents = 128\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We proceed to compile the PC with `pyjuice.compile`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pc = juice.compile(ns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `pc` is an instance of `torch.nn.Module`. So we can move it to the GPU as if it is a neural network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pc.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the PC\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by defining the optimizer and scheduler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = juice.optim.CircuitOptimizer(pc, lr = 0.1, pseudocount = 0.1, method = \"EM\")\nscheduler = juice.optim.CircuitScheduler(\n    optimizer, \n    method = \"multi_linear\", \n    lrs = [0.9, 0.1, 0.05], \n    milestone_steps = [0, len(train_loader) * 100, len(train_loader) * 350]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optionally, we can leverage CUDA Graphs to hide the kernel launching overhead by doing a dry run.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for batch in train_loader:\n    x = batch[0].to(device)\n\n    lls = pc(x, record_cudagraph = True)\n    lls.mean().backward()\n    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready for the training. Below is an example training loop for mini-batch EM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, 350+1):\n    t0 = time.time()\n    train_ll = 0.0\n    for batch in train_loader:\n        x = batch[0].to(device)\n\n        # Similar to PyTorch optimizers zeroling out the gradients, we zero out the parameter flows\n        optimizer.zero_grad()\n\n        # Forward pass\n        lls = pc(x)\n\n        # Backward pass\n        lls.mean().backward()\n\n        train_ll += lls.mean().detach().cpu().numpy().item()\n\n        # Perform a mini-batch EM step\n        optimizer.step()\n        scheduler.step()\n\n    train_ll /= len(train_loader)\n\n    t1 = time.time()\n    test_ll = 0.0\n    for batch in valid_loader:\n        x = batch[0].to(pc.device)\n        lls = pc(x)\n        test_ll += lls.mean().detach().cpu().numpy().item()\n    \n    test_ll /= len(valid_loader)\n    t2 = time.time()\n\n    print(f\"[Epoch {epoch}/{350}][train LL: {train_ll:.2f}; val LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; val forward {t2-t1:.2f}] \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, an example training loop for full-batch EM is given as follows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, 1+1):\n    t0 = time.time()\n\n    # Manually zeroling out the flows\n    pc.init_param_flows(flows_memory = 0.0)\n\n    train_ll = 0.0\n    for batch in train_loader:\n        x = batch[0].to(device)\n\n        # We only run the forward and the backward pass, and accumulate the flows throughout the epoch\n        lls = pc(x)\n        lls.mean().backward()\n\n        train_ll += lls.mean().detach().cpu().numpy().item()\n\n    # Set step size to 1.0 for full-batch EM\n    pc.mini_batch_em(step_size = 1.0, pseudocount = 0.01)\n\n    train_ll /= len(train_loader)\n\n    t1 = time.time()\n    test_ll = 0.0\n    for batch in valid_loader:\n        x = batch[0].to(pc.device)\n        lls = pc(x)\n        test_ll += lls.mean().detach().cpu().numpy().item()\n    \n    test_ll /= len(valid_loader)\n    t2 = time.time()\n    print(f\"[Epoch {epoch}/{1}][train LL: {train_ll:.2f}; val LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; val forward {t2-t1:.2f}] \")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}