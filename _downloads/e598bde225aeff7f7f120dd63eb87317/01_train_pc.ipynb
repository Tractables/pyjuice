{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Train a PC\n\nThis tutorial demonstrates how to create a Hidden Chow-Liu Tree (https://arxiv.org/pdf/2106.02264.pdf) using :code:`pyjuice.structures` and train the model with mini-batch EM and full-batch EM.\nFor simplicity, we use the MNIST dataset as an example. \n\nNote that the goal of this tutorial is just to quickly demonstrate the basic training pipeline using PyJuice without covering additional details such as ways to construct a PC, which will be covered in the following tutorials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = 'imgs/juice.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the MNIST Dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pyjuice as juice\nimport torch\nimport torchvision\nimport time\nfrom torch.utils.data import TensorDataset, DataLoader\nimport pyjuice.nodes.distributions as dists\n\ntrain_dataset = torchvision.datasets.MNIST(root = \"../data\", train = True, download = True)\nvalid_dataset = torchvision.datasets.MNIST(root = \"../data\", train = False, download = True)\n\ntrain_data = train_dataset.data.reshape(60000, 28*28)\nvalid_data = valid_dataset.data.reshape(10000, 28*28)\n\ntrain_loader = DataLoader(\n    dataset = TensorDataset(train_data),\n    batch_size = 512,\n    shuffle = True,\n    drop_last = True\n)\nvalid_loader = DataLoader(\n    dataset = TensorDataset(valid_data),\n    batch_size = 512,\n    shuffle = False,\n    drop_last = True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the PC\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a HCLT PC with latent size 128.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n\n# The data is required to construct the backbone Chow-Liu Tree structure for the HCLT\nns = juice.structures.HCLT(\n    train_data.float().to(device), \n    num_latents = 128\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":code:`ns` is a Directed Acyclic Graph (DAG) representation of the PC. \nSpecifically, we use :code:`pyjuice.nodes.InputNodes`, :code:`pyjuice.nodes.ProdNodes`, and :code:`pyjuice.nodes.SumNodes` to define vectors of input nodes, product nodes, and sum nodes, respectively.\nBy also storing the topological structure of the node vectors (with pointers to the child node vectors), we create the PC as a DAG-based structure. :code:`ns` is also just a node vector defining the root node of the PC.\n\nWhile being user-friendly, the DAG-based representation is not amenable to efficient computation. \nTherefore, before doing any computation, we need to compile the PC with :code:`pyjuice.compile`, which creates a compact and equivalent representation of the PC.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pc = juice.compile(ns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :code:`pc` is an instance of :code:`torch.nn.Module`. So we can safely assume it is just a neural network with the variable assignments $\\mathbf{x}$ as input and its log-likelihood $\\log p(\\mathbf{x})$ as output. \nWe proceed to move it to the GPU specified by :code:`device`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pc.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the PC\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by defining the optimizer and scheduler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = juice.optim.CircuitOptimizer(pc, lr = 0.1, pseudocount = 0.1, method = \"EM\")\nscheduler = juice.optim.CircuitScheduler(\n    optimizer, \n    method = \"multi_linear\", \n    lrs = [0.9, 0.1, 0.05], \n    milestone_steps = [0, len(train_loader) * 100, len(train_loader) * 350]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optionally, we can leverage CUDA Graphs to hide the kernel launching overhead by doing a dry run.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for batch in train_loader:\n    x = batch[0].to(device)\n\n    lls = pc(x, record_cudagraph = True)\n    lls.mean().backward()\n    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready for the training. Below is an example training loop for mini-batch EM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, 350+1):\n    t0 = time.time()\n    train_ll = 0.0\n    for batch in train_loader:\n        x = batch[0].to(device)\n\n        # Similar to PyTorch optimizers zeroling out the gradients, we zero out the parameter flows\n        optimizer.zero_grad()\n\n        # Forward pass\n        lls = pc(x)\n\n        # Backward pass\n        lls.mean().backward()\n\n        train_ll += lls.mean().detach().cpu().numpy().item()\n\n        # Perform a mini-batch EM step\n        optimizer.step()\n        scheduler.step()\n\n    train_ll /= len(train_loader)\n\n    t1 = time.time()\n    test_ll = 0.0\n    for batch in valid_loader:\n        x = batch[0].to(pc.device)\n        lls = pc(x)\n        test_ll += lls.mean().detach().cpu().numpy().item()\n    \n    test_ll /= len(valid_loader)\n    t2 = time.time()\n\n    print(f\"[Epoch {epoch}/{350}][train LL: {train_ll:.2f}; val LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; val forward {t2-t1:.2f}] \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, an example training loop for full-batch EM is given as follows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, 1+1):\n    t0 = time.time()\n\n    # Manually zeroling out the flows\n    pc.init_param_flows(flows_memory = 0.0)\n\n    train_ll = 0.0\n    for batch in train_loader:\n        x = batch[0].to(device)\n\n        # We only run the forward and the backward pass, and accumulate the flows throughout the epoch\n        lls = pc(x)\n        lls.mean().backward()\n\n        train_ll += lls.mean().detach().cpu().numpy().item()\n\n    # Set step size to 1.0 for full-batch EM\n    pc.mini_batch_em(step_size = 1.0, pseudocount = 0.01)\n\n    train_ll /= len(train_loader)\n\n    t1 = time.time()\n    test_ll = 0.0\n    for batch in valid_loader:\n        x = batch[0].to(pc.device)\n        lls = pc(x)\n        test_ll += lls.mean().detach().cpu().numpy().item()\n    \n    test_ll /= len(valid_loader)\n    t2 = time.time()\n    print(f\"[Epoch {epoch}/{1}][train LL: {train_ll:.2f}; val LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; val forward {t2-t1:.2f}] \")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}