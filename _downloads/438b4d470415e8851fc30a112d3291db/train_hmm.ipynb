{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Example\n\ndddd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = 'imgs/juice.png'\n\nimport pyjuice as juice\nimport torch\nimport torchvision\nimport time\nimport tqdm\nfrom torch.utils.data import TensorDataset, DataLoader\nimport pyjuice.nodes.distributions as dists\n\n\ndef evaluate(pc, loader):\n    lls_total = 0.0\n    for batch in loader:\n        x = batch[0].to(pc.device)\n        lls = pc(x)\n        lls_total += lls.mean().detach().cpu().numpy().item()\n    \n    lls_total /= len(loader)\n    return lls_total\n\n\ndef mini_batch_em_epoch(num_epochs, pc, optimizer, scheduler, train_loader, test_loader, device):\n    for epoch in range(num_epochs):\n        t0 = time.time()\n        train_ll = 0.0\n        for batch in train_loader:\n            x = batch[0].to(device)\n\n            optimizer.zero_grad()\n\n            lls = pc(x)\n            lls.mean().backward()\n\n            train_ll += lls.mean().detach().cpu().numpy().item()\n\n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n\n        train_ll /= len(train_loader)\n\n        t1 = time.time()\n        test_ll = evaluate(pc, loader=test_loader)\n        t2 = time.time()\n\n        print(f\"[Epoch {epoch}/{num_epochs}][train LL: {train_ll:.2f}; test LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; test forward {t2-t1:.2f}] \")\n\n\ndef full_batch_em_epoch(pc, train_loader, test_loader, device):\n\n    pc.init_param_flows(flows_memory = 0.0)\n\n    t0 = time.time()\n    train_ll = 0.0\n    for batch in tqdm.tqdm(train_loader):\n        x = batch[0].to(device)\n\n        lls = pc(x)\n        lls.mean().backward()\n\n        train_ll += lls.mean().detach().cpu().numpy().item()\n\n    pc.mini_batch_em(step_size = 1.0, pseudocount = 0.1)\n\n    train_ll /= len(train_loader)\n\n    t1 = time.time()\n    test_ll = evaluate(pc, loader=test_loader)\n    t2 = time.time()\n    print(f\"[train LL: {train_ll:.2f}; test LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; test forward {t2-t1:.2f}] \")\n\n\ndef homogenes_hmm(seq_length, num_latents, vocab_size):\n    \n    group_size = min(juice.utils.util.max_cdf_power_of_2(num_latents), 1024)\n    num_node_groups = num_latents // group_size\n    \n    with juice.set_group_size(group_size = group_size):\n        ns_input = juice.inputs(seq_length - 1, num_node_groups = num_node_groups,\n                                dist = dists.Categorical(num_cats = vocab_size))\n        \n        ns_sum = None\n        curr_zs = ns_input\n        for var in range(seq_length - 2, -1, -1):\n            curr_xs = ns_input.duplicate(var, tie_params = True)\n            \n            if ns_sum is None:\n                ns = juice.summate(\n                    curr_zs, num_node_groups = num_node_groups)\n                ns_sum = ns\n            else:\n                ns = ns_sum.duplicate(curr_zs, tie_params=True)\n\n            curr_zs = juice.multiply(curr_xs, ns)\n            \n        ns = juice.summate(curr_zs, num_node_groups = 1, group_size = 1)\n    \n    ns.init_parameters()\n    \n    return ns\n\n\ndef train_hmm(enable_cudagrph = True):\n\n    device = torch.device(\"cuda:0\")\n\n    T = 32\n    ns = homogenes_hmm(T, 8192, 4023)\n    \n    pc = juice.TensorCircuit(ns, max_tied_ns_per_parflow_group = 2)\n    pc.print_statistics()\n\n    pc.to(device)\n\n    data = torch.randint(0, 10000, (6400, T))\n\n    data_loader = DataLoader(\n        dataset = TensorDataset(data),\n        batch_size = 64,\n        shuffle = True,\n        drop_last = True\n    )\n\n    optimizer = juice.optim.CircuitOptimizer(pc, lr = 0.1, pseudocount = 0.0001)\n\n    for batch in tqdm.tqdm(data_loader):\n        x = batch[0].to(device)\n\n        lls = pc(x)\n        lls.mean().backward()\n\n        break\n\n    torch.cuda.synchronize()\n    t0 = time.time()\n\n    for batch in tqdm.tqdm(data_loader):\n        x = batch[0].to(device)\n\n        lls = pc(x)\n        lls.mean().backward()\n\n    torch.cuda.synchronize()\n    t1 = time.time()\n\n    print((t1-t0)/100*1000, \"ms\")\n\n    # mini_batch_em_epoch(350, pc, optimizer, None, data_loader, data_loader, device)\n\n\nif __name__ == \"__main__\":\n    train_hmm()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}