
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/01_train_pc.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_01_train_pc.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_01_train_pc.py:


Train a PC
==========

This tutorial demonstrates how to create a Hidden Chow-Liu Tree (https://arxiv.org/pdf/2106.02264.pdf) using `pyjuice.structures` and train the model with mini-batch EM and full-batch EM.

For simplicity, we use the MNIST dataset as an example.

.. GENERATED FROM PYTHON SOURCE LINES 9-12

.. code-block:: Python


    # sphinx_gallery_thumbnail_path = 'imgs/juice.png'


.. GENERATED FROM PYTHON SOURCE LINES 13-15

Load the MNIST Dataset
----------------------

.. GENERATED FROM PYTHON SOURCE LINES 15-42

.. code-block:: Python


    import pyjuice as juice
    import torch
    import torchvision
    import time
    from torch.utils.data import TensorDataset, DataLoader
    import pyjuice.nodes.distributions as dists

    train_dataset = torchvision.datasets.MNIST(root = "../data", train = True, download = True)
    valid_dataset = torchvision.datasets.MNIST(root = "../data", train = False, download = True)

    train_data = train_dataset.data.reshape(60000, 28*28)
    valid_data = valid_dataset.data.reshape(10000, 28*28)

    train_loader = DataLoader(
        dataset = TensorDataset(train_data),
        batch_size = 512,
        shuffle = True,
        drop_last = True
    )
    valid_loader = DataLoader(
        dataset = TensorDataset(valid_data),
        batch_size = 512,
        shuffle = False,
        drop_last = True
    )


.. GENERATED FROM PYTHON SOURCE LINES 43-45

Create the PC
-------------

.. GENERATED FROM PYTHON SOURCE LINES 47-48

Let's create a HCLT PC with latent size 128.

.. GENERATED FROM PYTHON SOURCE LINES 48-57

.. code-block:: Python


    device = torch.device("cuda:0")

    # The data is required to construct the backbone Chow-Liu Tree structure for the HCLT
    ns = juice.structures.HCLT(
        train_data.float().to(device), 
        num_latents = 128
    )


.. GENERATED FROM PYTHON SOURCE LINES 58-59

We proceed to compile the PC with `pyjuice.compile`.

.. GENERATED FROM PYTHON SOURCE LINES 59-62

.. code-block:: Python


    pc = juice.compile(ns)


.. GENERATED FROM PYTHON SOURCE LINES 63-64

The `pc` is an instance of `torch.nn.Module`. So we can move it to the GPU as if it is a neural network.

.. GENERATED FROM PYTHON SOURCE LINES 64-67

.. code-block:: Python


    pc.to(device)


.. GENERATED FROM PYTHON SOURCE LINES 68-70

Train the PC
------------

.. GENERATED FROM PYTHON SOURCE LINES 72-73

We start by defining the optimizer and scheduler.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

.. code-block:: Python


    optimizer = juice.optim.CircuitOptimizer(pc, lr = 0.1, pseudocount = 0.1, method = "EM")
    scheduler = juice.optim.CircuitScheduler(
        optimizer, 
        method = "multi_linear", 
        lrs = [0.9, 0.1, 0.05], 
        milestone_steps = [0, len(train_loader) * 100, len(train_loader) * 350]
    )


.. GENERATED FROM PYTHON SOURCE LINES 83-84

Optionally, we can leverage CUDA Graphs to hide the kernel launching overhead by doing a dry run.

.. GENERATED FROM PYTHON SOURCE LINES 84-92

.. code-block:: Python


    for batch in train_loader:
        x = batch[0].to(device)

        lls = pc(x, record_cudagraph = True)
        lls.mean().backward()
        break


.. GENERATED FROM PYTHON SOURCE LINES 93-94

We are now ready for the training. Below is an example training loop for mini-batch EM.

.. GENERATED FROM PYTHON SOURCE LINES 94-130

.. code-block:: Python


    for epoch in range(1, 350+1):
        t0 = time.time()
        train_ll = 0.0
        for batch in train_loader:
            x = batch[0].to(device)

            # Similar to PyTorch optimizers zeroling out the gradients, we zero out the parameter flows
            optimizer.zero_grad()

            # Forward pass
            lls = pc(x)

            # Backward pass
            lls.mean().backward()

            train_ll += lls.mean().detach().cpu().numpy().item()

            # Perform a mini-batch EM step
            optimizer.step()
            scheduler.step()

        train_ll /= len(train_loader)

        t1 = time.time()
        test_ll = 0.0
        for batch in valid_loader:
            x = batch[0].to(pc.device)
            lls = pc(x)
            test_ll += lls.mean().detach().cpu().numpy().item()
    
        test_ll /= len(valid_loader)
        t2 = time.time()

        print(f"[Epoch {epoch}/{350}][train LL: {train_ll:.2f}; val LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; val forward {t2-t1:.2f}] ")


.. GENERATED FROM PYTHON SOURCE LINES 131-132

Similarly, an example training loop for full-batch EM is given as follows.

.. GENERATED FROM PYTHON SOURCE LINES 132-164

.. code-block:: Python


    for epoch in range(1, 1+1):
        t0 = time.time()

        # Manually zeroling out the flows
        pc.init_param_flows(flows_memory = 0.0)

        train_ll = 0.0
        for batch in train_loader:
            x = batch[0].to(device)

            # We only run the forward and the backward pass, and accumulate the flows throughout the epoch
            lls = pc(x)
            lls.mean().backward()

            train_ll += lls.mean().detach().cpu().numpy().item()

        # Set step size to 1.0 for full-batch EM
        pc.mini_batch_em(step_size = 1.0, pseudocount = 0.01)

        train_ll /= len(train_loader)

        t1 = time.time()
        test_ll = 0.0
        for batch in valid_loader:
            x = batch[0].to(pc.device)
            lls = pc(x)
            test_ll += lls.mean().detach().cpu().numpy().item()
    
        test_ll /= len(valid_loader)
        t2 = time.time()
        print(f"[Epoch {epoch}/{1}][train LL: {train_ll:.2f}; val LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; val forward {t2-t1:.2f}] ")


.. _sphx_glr_download_getting-started_tutorials_01_train_pc.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 01_train_pc.ipynb <01_train_pc.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 01_train_pc.py <01_train_pc.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
