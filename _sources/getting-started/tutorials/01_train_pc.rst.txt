
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/01_train_pc.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_01_train_pc.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_01_train_pc.py:


Train a PC
==========

This tutorial demonstrates how to create a Hidden Chow-Liu Tree (https://arxiv.org/pdf/2106.02264.pdf) using `pyjuice.structures` and train the model with mini-batch EM and full-batch EM.

For simplicity, we use the MNIST dataset as an example. 
Note that the goal of this tutorial is just to quickly demonstrate the basic training pipeline using PyJuice without covering additional details about details such as ways to construct a PC, which will be covered in the following tutorials.

.. GENERATED FROM PYTHON SOURCE LINES 12-14

Load the MNIST Dataset
----------------------

.. GENERATED FROM PYTHON SOURCE LINES 14-41

.. code-block:: Python


    import pyjuice as juice
    import torch
    import torchvision
    import time
    from torch.utils.data import TensorDataset, DataLoader
    import pyjuice.nodes.distributions as dists

    train_dataset = torchvision.datasets.MNIST(root = "../data", train = True, download = True)
    valid_dataset = torchvision.datasets.MNIST(root = "../data", train = False, download = True)

    train_data = train_dataset.data.reshape(60000, 28*28)
    valid_data = valid_dataset.data.reshape(10000, 28*28)

    train_loader = DataLoader(
        dataset = TensorDataset(train_data),
        batch_size = 512,
        shuffle = True,
        drop_last = True
    )
    valid_loader = DataLoader(
        dataset = TensorDataset(valid_data),
        batch_size = 512,
        shuffle = False,
        drop_last = True
    )


.. GENERATED FROM PYTHON SOURCE LINES 42-44

Create the PC
-------------

.. GENERATED FROM PYTHON SOURCE LINES 46-47

Let's create a HCLT PC with latent size 128.

.. GENERATED FROM PYTHON SOURCE LINES 47-56

.. code-block:: Python


    device = torch.device("cuda:0")

    # The data is required to construct the backbone Chow-Liu Tree structure for the HCLT
    ns = juice.structures.HCLT(
        train_data.float().to(device), 
        num_latents = 128
    )


.. GENERATED FROM PYTHON SOURCE LINES 57-58

We proceed to compile the PC with `pyjuice.compile`.

.. GENERATED FROM PYTHON SOURCE LINES 58-61

.. code-block:: Python


    pc = juice.compile(ns)


.. GENERATED FROM PYTHON SOURCE LINES 62-63

The `pc` is an instance of `torch.nn.Module`. So we can move it to the GPU as if it is a neural network.

.. GENERATED FROM PYTHON SOURCE LINES 63-66

.. code-block:: Python


    pc.to(device)


.. GENERATED FROM PYTHON SOURCE LINES 67-69

Train the PC
------------

.. GENERATED FROM PYTHON SOURCE LINES 71-72

We start by defining the optimizer and scheduler.

.. GENERATED FROM PYTHON SOURCE LINES 72-81

.. code-block:: Python


    optimizer = juice.optim.CircuitOptimizer(pc, lr = 0.1, pseudocount = 0.1, method = "EM")
    scheduler = juice.optim.CircuitScheduler(
        optimizer, 
        method = "multi_linear", 
        lrs = [0.9, 0.1, 0.05], 
        milestone_steps = [0, len(train_loader) * 100, len(train_loader) * 350]
    )


.. GENERATED FROM PYTHON SOURCE LINES 82-83

Optionally, we can leverage CUDA Graphs to hide the kernel launching overhead by doing a dry run.

.. GENERATED FROM PYTHON SOURCE LINES 83-91

.. code-block:: Python


    for batch in train_loader:
        x = batch[0].to(device)

        lls = pc(x, record_cudagraph = True)
        lls.mean().backward()
        break


.. GENERATED FROM PYTHON SOURCE LINES 92-93

We are now ready for the training. Below is an example training loop for mini-batch EM.

.. GENERATED FROM PYTHON SOURCE LINES 93-129

.. code-block:: Python


    for epoch in range(1, 350+1):
        t0 = time.time()
        train_ll = 0.0
        for batch in train_loader:
            x = batch[0].to(device)

            # Similar to PyTorch optimizers zeroling out the gradients, we zero out the parameter flows
            optimizer.zero_grad()

            # Forward pass
            lls = pc(x)

            # Backward pass
            lls.mean().backward()

            train_ll += lls.mean().detach().cpu().numpy().item()

            # Perform a mini-batch EM step
            optimizer.step()
            scheduler.step()

        train_ll /= len(train_loader)

        t1 = time.time()
        test_ll = 0.0
        for batch in valid_loader:
            x = batch[0].to(pc.device)
            lls = pc(x)
            test_ll += lls.mean().detach().cpu().numpy().item()
    
        test_ll /= len(valid_loader)
        t2 = time.time()

        print(f"[Epoch {epoch}/{350}][train LL: {train_ll:.2f}; val LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; val forward {t2-t1:.2f}] ")


.. GENERATED FROM PYTHON SOURCE LINES 130-131

Similarly, an example training loop for full-batch EM is given as follows.

.. GENERATED FROM PYTHON SOURCE LINES 131-163

.. code-block:: Python


    for epoch in range(1, 1+1):
        t0 = time.time()

        # Manually zeroling out the flows
        pc.init_param_flows(flows_memory = 0.0)

        train_ll = 0.0
        for batch in train_loader:
            x = batch[0].to(device)

            # We only run the forward and the backward pass, and accumulate the flows throughout the epoch
            lls = pc(x)
            lls.mean().backward()

            train_ll += lls.mean().detach().cpu().numpy().item()

        # Set step size to 1.0 for full-batch EM
        pc.mini_batch_em(step_size = 1.0, pseudocount = 0.01)

        train_ll /= len(train_loader)

        t1 = time.time()
        test_ll = 0.0
        for batch in valid_loader:
            x = batch[0].to(pc.device)
            lls = pc(x)
            test_ll += lls.mean().detach().cpu().numpy().item()
    
        test_ll /= len(valid_loader)
        t2 = time.time()
        print(f"[Epoch {epoch}/{1}][train LL: {train_ll:.2f}; val LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; val forward {t2-t1:.2f}] ")


.. _sphx_glr_download_getting-started_tutorials_01_train_pc.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 01_train_pc.ipynb <01_train_pc.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 01_train_pc.py <01_train_pc.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
